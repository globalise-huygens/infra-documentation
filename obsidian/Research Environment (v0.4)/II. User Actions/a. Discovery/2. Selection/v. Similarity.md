#### Images

We've not yet experimented or even discussed the possibility of using the page images themselves as a query input. For example, "show me more pages that look like these pages" (e.g. for ad hoc visual features that aren't worth it to process in advance as filterable/facetted enrichments, https://transcriptions.globalise.huygens.knaw.nl/detail/urn:globalise:NL-HaNA_1.04.02_3477_0711).

This could be done, for example, using image embeddings, or multi-modal text/image embedding (CLIP). However, given that a) we've heard almost no requests for this from our users, b) we have no experience with this in-house, c) no (?) existing multi-modal models were trained on manuscript images and metadata relevant to us and are ready-to-use, d) there are a many different technical strategies we would need to explore to select the optimum model and embedding strategy, this might be something we'll need to set aside or defer until later in the project, and then first approach it as an experimental lab.

#### Texts

Some possibilities:

Embeddings
Hybrid
RAG